<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Memory Automaton: Rainbow AliveCount</title>
  <link rel="icon" type="image/x-icon" href="favicon.ico">
  <style>
    /* Basic reset and styling */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      font-family: "Helvetica Neue", Arial, sans-serif;
      color: #fff;        /* White text for main page content */
      background: #000;   /* Black page background */
      overflow-x: hidden;  /* Hide horizontal scrollbar only */
      overflow-y: auto;    /* Allow vertical scrolling */
    }

    .content {
      position: relative;
      z-index: 1;

      /* 40% wider for better readability */
      max-width: 980px;

      /* 25% bigger margins & padding than before */
      margin: 2.5rem auto;
      padding: 2.5rem 1.5rem 4rem;

      /* Darker background panel for clarity */
      background: rgba(0,0,0,0.75);
      border-radius: 8px;
    }

    /* For smaller screens, preserve 5% margins on each side */
    @media (max-width: 768px) {
      .content {
        margin: 2.5rem 5%;
      }
    }

    h1 {
      /* Slightly tighter spacing */
      font-size: 3rem;
      margin-bottom: 1.2rem;
      text-align: center;
      line-height: 1.2;
    }

    p, ul {
      /* Tighter line spacing */
      font-size: 1.25rem;
      line-height: 1.5;
      margin-bottom: 1rem;
    }

    @media (min-width: 768px) {
      h1 {
        font-size: 3.6rem;
      }
      p, ul {
        font-size: 1.4rem;
      }
    }

    /* Make images fill 75% of their container width and center them */
    img {
      width: 75%;
      height: auto;
      margin: 1.5rem auto 0.75rem;
      display: block;
    }

    /* Center figure captions and add spacing */
    figure {
      margin: 2rem 0 3rem;
      text-align: center;
    }

    figcaption {
      margin-top: 1rem;
      font-size: 1.1rem;
      opacity: 0.9;
    }

    /* Style links to match text color */
    a {
      color: #fff;
      text-decoration: underline;
    }

    a:visited {
      color: #cccccc;
    }

    /* Right-aligned TL;DR section */
    .tldr {
      text-align: right;
      margin-bottom: 2rem;
    }

    /* The canvas sits behind everything */
    #automatonCanvas {
      position: fixed;
      top: 0;
      left: 0;
      z-index: 0;
    }
  </style>
</head>

<body>

  <div class="content">

<p>TL;DR:</p>
<a href="#generality">General intelligence ≈ universal
computation.</a>
<a href="https://arxiv.org/abs/2412.17794">Universality only
requires reliable thinking and memory of your thoughts.</a>
<a href="#bronzeage">Your average human with a pen and paper can
compute anything computable, powered only by sandwiches.</a>
<a href="#attention">Current AI achieves this through transformer
attention—at a quadratic cost thats on pace to eat the worlds venture
capital and boil our oceans.</a>
<a href="https://arxiv.org/abs/2410.01201">We can do better.</a>
<a href="#crispr">CRISPR shows us that, when efficient universality
is needed, state appears.</a>
<a href="#recurrence">The recurrent nature of natural intelligence
suggests a way forward.</a>
<a href="https://arxiv.org/abs/2412.17794">Be true to yourself, and
remember what you’ve done.</a>
<p>
<hr />
<p>On a morning in early December I was running by the Mississippi when
I saw a large industrial plume on the horizon.</p>
<figure>
<img src="xai-plume-memphis.jpg"
alt="xAI’s cooling plume as seen from Mud Island in Memphis" />
<figcaption aria-hidden="true">xAI’s cooling plume as seen from Mud
Island in Memphis</figcaption>
</figure>
<p>The plume is water vapor and nitrous oxides from what is believed to
be the largest supercomputer in the world, xAI’s “Colossus”. 100k H100s
are vaporizing 5 million liters of water a day from the Memphis sands
aquifer. Dozens of portable methane generators feed the system with at
least 100 MW—enough for 50,000 homes. xAI couldn’t wait for the
Tennessee Valley Authority to build new capacity; that delay might mean
losing the race for a quantum leap in human capability.</p>
<p>This plume, now dominating Memphis’s skyline, is compute’s heat waste
made visible in our rush to train an artificial general intelligence
(AGI). <a
href="https://www.capacitymedia.com/article/musks-xais-colossus-cluster-set-for-one-million-gpu-supercomputer-expansion">xAI’s
stated goal is a “1% shot at a Kardashev type 1 civilization”</a>—a <a
href="https://en.wikipedia.org/wiki/Planetary_civilization">society
capable of harnessing all its planet’s energy</a>, presumably enabled by
the AGI they aim to bring into the world. And they’re just getting
started, with a tenfold <a
href="https://www.ft.com/content/9c0516cf-dd12-4665-aa22-712de854fe2f">expansion
of Colossus to 1M GPUs</a> already planned.</p>
<p>You may have noticed that, although ravenous and hyperactive, a human
child doesn’t generate a thousand meter plume of water vapor and
reactive particulates to develop their intelligence. Yet your average
human, fueled by sandwiches, can implement any algorithm with just pen
and paper. <a name="bronzeage">So augmented with bronze age technology,
we become “universal” computers in Turing’s language—the quintessential
“general” intelligence capable of solving any problem given enough time
and snacks.</a> For the sake of argument, let’s agree: general
intelligence ≈ universal computer.</p>
<p>That link has been on my mind for two years, ever since I first read
the magic words “Let’s think step by step” in a groundbreaking machine
learning paper. I was amused by how a simple phrase seemed to invoke a
more deliberate, thoughtful mode in large language models (LLMs). It was
as if some psychological “effect” flipped them into a more rigorous and
structured part of their state space, leading to dramatic improvement in
performance, and encouraging the world’s LLM whisperers to add this
incantation to every time they saw models faltering.</p>
<p>But theoretical analyses of transformers revealed something deeper
than just a neat psychology trick. Transformers, in a single forward
pass, are restricted to a class called TC0, basically a family of
threshold circuits that can be computed in parallel with aggregation.
TC0 systems are often called “bounded parallel”. They can quickly match
patterns, but can’t count, do math, or follow a recipe unless the
process can be encoded in a fixed set of circuits. <em>However</em>, if
you use “chain-of-thought” or a series of recursive prompts—feeding each
of the model’s outputs back into its input—the combined system becomes
capable of simulating a universal Turing machine. The model’s
expressivity, harnessed across multiple steps, blossoms into general
computation.</p>
<p>Out of deep love for this line of research, I spent much of December
in a meditation on the link between memory and computation. This post is
a human introduction to that paper, where I provide the simplest and
most intuitive explanation of universality (a.k.a. general intelligence)
that I could synthesize. <a name="generality">Universality requires only
(1) stable evolution of thought (no hallucinations), and (2) reliable
access to the history of thought.</a> This synthesis rightly seems
banal, dull, too simple to be useful, too theory of computation 101 to
be interesting. And OK, if you studied the theory of computation, yes,
it is, but why did y’all forget the textbook and start imaging that
threshold circuits and feedforward networks could achieve general
intelligence? If you can remember, this perspective quickly takes us to
some amazing places.</p>
<p>The first place is an intuitive understanding of what LLMs are. <a
href="https://en.wikipedia.org/wiki/Neural_scaling_law">Machine learning
got excited by the observation that adding more parameters and training
tokens could yield “scaling laws” that took us to a place where
intelligence “emerged”</a>. Another interpretation is that LLMs are a
quintessential example of transfer learning, where humans and our
society is the model that’s being transferred. By trying to predict what
humans might say or do next, the models effectively learned patterns
embedded in our writing—including the latent computational rules that
shape human reasoning. They’re not just modeling words but the programs
behind those words (which might be human minds, or other models, or
entire cryptographic protocols). Looking through the lens of Turing
machines, we’ve transferred our state transition functions to them,
which they are now they are able to generalize to new input contexts (<a
href="https://arxiv.org/abs/2303.12712">drawing tikz unicorns and
such</a>). This deeper transfer of computational thinking <a
href="https://ai.meta.com/blog/meta-llama-3/">explains why models
continue improving far beyond naive pattern-matching saturation
points</a>—they’re gradually refining their models of human cognitive
processes themselves.</p>
<p>Part of the magic is that LLMs, must of which are Transformers, are
forging a shared computational interface with us, letting us push each
other around in a collaborative workspace space that can solve new
tasks. However, they have a costly flaw that makes them otherworldly in
their resource demands: <a name="attention">attention</a>. In effect,
attention is a learned function for memory and association across the
input. But with n tokens, each must attend to every other token—an n × n
explosion in both compute and memory. Double your context length,
quadruple the cost. Want to “think” four times longer? That’s sixteen
times more overhead. And because memory is the key to intelligence,
model context length must be increased, leading to quadratic increases
in costs which I saw made visible in Memphis last month. Attention
demonstrates the bitter lesson: end-to-end learning beats architecture,
but it also forces us to eat a bitter pill: a quadratic cost that we
can’t scale forever. A transformer’s quadratic scaling hits a wall at 1M
tokens:</p>
<figure>
<img src="quadratic-wall.png"
alt="Comparing linear, log-linear, and quadratic scaling vs. input." />
<figcaption aria-hidden="true">Comparing linear, log-linear, and
quadratic scaling vs. input.</figcaption>
</figure>
<p>But the truth is, generality doesn’t require a quadratic cost. Look
at a single cell. It has gigabases of DNA and a swarm of biomolecules,
operating in a parallel soup with no centralizing control. By some
definitions, each cell’s molecular interactions also fall in that
threshold-circuit-like class: essentially parallel transformations, no
large sequential pipeline. But cells offset this limitation with
state—encoded in their components and their DNA, which holds a record of
life’s entire lineage. When cells need to store or recall specific bits
of information, they literally write it into the genome.
<a name="crispr">The canonical example is CRISPR, which can be
understood most fundamentally as a chronological log of fragments of
viruses, addresses in the space of DNA, that represent viral infections
that this particular cell has survived, and its lineage has
survived.</a> The chronology is not just an artifact of the system, but
is essential because by remembering which viruses are more recent, the
cell can invest more energy in making guided nucleases that target them
than ones which occurred long ago.</p>
<figure>
<img src="crispr-memory.png"
alt="CRISPR, better known for its use as a genome editing tool, is actually a temporal memory system." />
<figcaption aria-hidden="true">CRISPR, better known for its use as a
genome editing tool, is actually a temporal memory system.</figcaption>
</figure>
<p>The ARC AGI Prize nicely illustrates where today’s models stand. A
single forward-pass language model—like GPT-4—gets about 0–5% accuracy
on ARC tasks. Cheaper chain-of-thought–based systems that do a bit of <a
href="https://arxiv.org/abs/2411.07279">test-time training</a> <a
href="https://arxiv.org/abs/2412.04604">get close to 50%</a>. Meanwhile,
a careful human solver will approach 100% if you just give them enough
time to think.</p>
<p>Then, in late December 2024, along comes <a
href="https://openai.com/index/deliberative-alignment/">OpenAI’s o3</a>,
which <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">nearly
solved the entire benchmark</a>. If you let the model “think less”—what
they call high-efficiency mode—it answers 400 tasks at 82.8% accuracy
for around $6,600. But if you let it think 172 times longer, exploring
many more possible solution paths in parallel, it climbs to 91.5%. That
final push costs an estimated $1.15 million (≈ 172 × $6.6k) in total
compute. <a href="https://openai.com/index/deliberative-alignment/">o3’s
authors describe this not primarily as an engineering hack but as an
ethical one</a>, “deliberative alignment” wherein an overseer system
that maintains the alignment of an kind of “forest of thought”, MCMC
sampled chains-of-thought which are kept in check. Their published
numbers suggest that they pay a monstrous cost to keep all those partial
reasoning traces in memory, to preserve consistency, and let the model
think longer in pursuit of another apparent scaling law.</p>
<p>In effect, by brute-forcing memory into reliability, they show how
easily you can approach universal computation if you’re willing to burn
a fortune on tokens. And this isn’t like previous scaling laws that
revealed new capabilities—it’s paying an enormous price to achieve
something that biological systems do routinely. To reach the kind of
sustained reliable thinking needed for real breakthroughs—novel cancer
treatments, scientific discoveries, fundamental insights—we’d need to
maintain computational state not just for a few more steps, but across
weeks or months of focused investigation. Not just in one mind, but
across communities of thinkers building on each other’s work. At
quadratic cost, that’s not just unsustainable it’s impossible.</p>
<p>Meanwhile, human civilization achieves this kind of extended
cognition through more elegant means. Ideas evolve across conversations,
papers, and experiments, maintaining state through distributed networks
of sandwich-powered minds working in shared contexts.</p>
<figure>
<img src="youssef_text_wbb.small.jpg"
alt="Written memory persists: A papyrus from Herculaneum, 2000 years old, decoded by a space-time transformer model. This work is close to my heart: I’m originally from Kentucky, where the Vesivius Challenge that led to this breakthrough is organized. I also spent nearly a decade living in Napoli, and have visited Ercolano scavi (Herculaneum) many times. Copyright Vesuvius Challenge." />
<figcaption aria-hidden="true">Written memory persists: A papyrus from
Herculaneum, 2000 years old, decoded by <a
href="https://arxiv.org/abs/2102.05095">a space-time transformer
model</a>. This work is close to my heart: I’m originally from Kentucky,
where the <a href="https://scrollprize.org/grandprize">Vesivius
Challenge</a> that led to this breakthrough is organized. I also spent
nearly a decade living in Napoli, and have visited Ercolano scavi
(Herculaneum) many times. Copyright Vesuvius Challenge.</figcaption>
</figure>
<p>Current AI research has thus discovered that “thinking longer” helps,
but in their excitement over this empirical finding, many miss that
reliable memory is the fundamental capability that makes computation
universal. The field has <a
href="https://www.youtube.com/watch?v=jPluSXJpdrA">embraced
psychological concepts like Daniel Kahneman “System 1” (fast, intuitive)
versus “System 2” (slow, deliberate) thinking to explain these
improvements</a>. o1/o3’s developers advocate <a
href="https://www.youtube.com/watch?v=eaAonE58sLU">dramatically scaling
test-time compute</a> to “hours, days, even weeks”, but I truly wonder
how quadratic-memory transformer-based architectures will allow this to
happen.</p>
<p>Soon after <a
href="https://openai.com/index/deliberative-alignment/">o3’s
release</a>, OpenAI’s Board said that to achieve AGI, we’ll just need
(<a href="https://daringfireball.net/2024/12/openai_unimaginable">as
paraphrased</a>) “unimaginable sums of money”, <a
href="https://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission/">proposing
to restructure as a public benefit corporation to attract fresh
capital</a>. They plan to scale up, spin more GPUs, and chase universal
intelligence through raw power.</p>
<p><a name="recurrence">As a sandwich-eating universal intelligence, I’m
not convinced.</a> Humans got smarter not by growing our brains, but by
distributing memory across society—spoken word, libraries, the
internet—finding ways to preserve and share knowledge without a ruinous
overhead. I’ll bet on a resurgence of <a
href="https://arxiv.org/html/2410.01201v2">recurrent approaches</a> that
drastically reduce inference costs by maintaining a <a
href="https://arxiv.org/abs/2412.06769">rolling hidden state,
incrementally processing new information, thinking slowly and deeply
when we need them to</a>). Yes, we might need specialized modules for
robust memory access—but if biology, evolution, and a good pen-and-paper
can do it, so can we. Generality doesn’t require industrial-scale
cooling towers and power plants. It just needs a reliable way to
remember and recall what you’ve thought.</p>
<p>Let’s remember.</p>


  </div>

<!-- p5.js from CDN -->
<script src="https://cdn.jsdelivr.net/npm/p5@1.4.2/lib/p5.min.js"></script>
<script>
/*
  Memory Automaton w/ Rainbow AliveCount (All Dead = Black)

  Changes:
    - All dead cells => black (0,0,0), regardless of mem.
    - If alive => pick from rainbow palette based on aliveCount.
    - If aliveCount=0 => no digit displayed (so first time doesn't show "0").
    - Text panel is bigger & darker.

  Each cell has:
    state (0/1),
    mem (0/1),
    aliveCount (0..9),
    colorVal = {r, g, b} ~ current displayed color (fades)
*/

// Cell geometry
let cellSize = 14;
let cols, rows;

// Each cell: { state, mem, aliveCount, colorVal: {r, g, b} }
let grid = [];

// Update speeds
let frameRateVal    = 10;
let updatesPerFrame = 20;
let patchSize       = 3;

// We define a subdued rainbow palette for aliveCount=0..9
const rainbowPalette = [
  { r: 160, g: 40,  b: 40  }, // 0  (dull red)
  { r: 180, g: 80,  b: 40  }, // 1  (soft orange)
  { r: 190, g: 140, b: 50  }, // 2  (soft yellow)
  { r: 90,  g: 160, b: 70  }, // 3  (pastel green)
  { r: 70,  g: 160, b: 140 }, // 4  (teal)
  { r: 70,  g: 100, b: 180 }, // 5  (blue)
  { r: 110, g: 70,  b: 180 }, // 6  (purple)
  { r: 150, g: 60,  b: 180 }, // 7  (lavender)
  { r: 180, g: 60,  b: 120 }, // 8  (magenta)
  { r: 180, g: 60,  b: 80  }  // 9  (pinkish-red)
];

// All dead cells => black
const blackColor = { r: 0, g: 0, b: 0 };

// Lerp rate for color transitions
const colorLerpRate = 0.08;

function setup() {
  createCanvas(windowWidth, windowHeight).id("automatonCanvas");
  frameRate(frameRateVal);

  cols = floor(width / cellSize);
  rows = floor(height / cellSize);

  textAlign(CENTER, CENTER);
  textSize(cellSize * 0.5);
  noStroke();

  // Initialize the grid
  for (let r = 0; r < rows; r++) {
    grid[r] = [];
    for (let c = 0; c < cols; c++) {
      let isAlive = random() < 0.15 ? 1 : 0;
      // Start color: either rainbowPalette[0] or black
      let colorStart = isAlive ? rainbowPalette[0] : blackColor;
      grid[r][c] = {
        state:      isAlive,
        mem:        0,
        aliveCount: 0,
        everAlive:  isAlive, // true if starting alive
        colorVal:   { r: colorStart.r, g: colorStart.g, b: colorStart.b }
      };
    }
  }
}

function draw() {
  // Semi-opaque black background => clears old frames gently
  background(0, 100);

  // Update random patches
  for (let i = 0; i < updatesPerFrame; i++) {
    let rr = floor(random(rows));
    let cc = floor(random(cols));
    updatePatch(rr, cc, patchSize);
  }

  // Render each cell
  for (let r = 0; r < rows; r++) {
    for (let c = 0; c < cols; c++) {
      let cell = grid[r][c];
      let target = getTargetColor(cell);

      // Smoothly lerp colorVal => target
      cell.colorVal.r = lerp(cell.colorVal.r, target.r, colorLerpRate);
      cell.colorVal.g = lerp(cell.colorVal.g, target.g, colorLerpRate);
      cell.colorVal.b = lerp(cell.colorVal.b, target.b, colorLerpRate);

      // Fill with the updated color
      fill(cell.colorVal.r, cell.colorVal.g, cell.colorVal.b);
      rect(c * cellSize, r * cellSize, cellSize, cellSize);

      // Show digit if aliveCount > 0 or if it's wrapped back to 0 (not first time)
      if (cell.aliveCount > 0 || (cell.aliveCount === 0 && cell.everAlive)) {
        fill(cell.colorVal.r * 0.7, cell.colorVal.g * 0.7, cell.colorVal.b * 0.7);
        text(
          cell.aliveCount,
          c * cellSize + cellSize / 2,
          r * cellSize + cellSize / 2
        );
      }
    }
  }
}

// Decide the target color for a cell
function getTargetColor(cell) {
  // If alive => pick from rainbowPalette based on aliveCount
  if (cell.state === 1) {
    return rainbowPalette[cell.aliveCount];
  }
  // Otherwise (dead) => black
  return blackColor;
}

// Update a patch of cells
function updatePatch(row, col, size) {
  for (let rr = row; rr < row + size; rr++) {
    for (let cc = col; cc < col + size; cc++) {
      if (rr < 0 || rr >= rows || cc < 0 || cc >= cols) continue;
      updateCell(rr, cc);
    }
  }
}

// Single-cell update logic
function updateCell(r, c) {
  let cell = grid[r][c];

  function safeGet(rr, cc) {
    if (rr < 0 || rr >= rows || cc < 0 || cc >= cols) return 0;
    return grid[rr][cc].state;
  }

  let left  = safeGet(r, c - 1);
  let self  = cell.state;
  let right = safeGet(r, c + 1);

  let newMem = cell.mem || self;  // if ever alive => mem=1

  // Standard "Rule 110" bits => index 0..7
  let neighborhood = (left << 2) | (self << 1) | right;
  let rule110 = [0,1,1,1,1,0,1,0];
  let outcome = rule110[ neighborhood ];

  // Twist: if mem=1 && neighborhood=2 => flip outcome
  if (newMem === 1 && neighborhood === 2) {
    outcome = outcome === 1 ? 0 : 1;
  }

  // If cell is alive => increment aliveCount (wrap at 9)
  let newCount = cell.aliveCount;
  if (outcome === 1) {
    newCount = (newCount + 1) % 10;
    cell.everAlive = true; // Mark as having been alive
  }

  cell.state      = outcome;
  cell.mem        = newMem;
  cell.aliveCount = newCount;
}

function windowResized() {
  resizeCanvas(windowWidth, windowHeight);
  cols = floor(width / cellSize);
  rows = floor(height / cellSize);

  textSize(cellSize * 0.5);

  // Rebuild grid, preserving old data
  let newGrid = [];
  for (let r = 0; r < rows; r++) {
    newGrid[r] = [];
    for (let c = 0; c < cols; c++) {
      if (grid[r] && grid[r][c]) {
        let old = grid[r][c];
        newGrid[r][c] = {
          state:      old.state,
          mem:        old.mem,
          aliveCount: old.aliveCount,
          everAlive:  old.everAlive,
          // copy over colorVal
          colorVal:   { 
            r: old.colorVal.r, 
            g: old.colorVal.g, 
            b: old.colorVal.b 
          }
        };
      } else {
        // If newly created cell is outside old range
        let isAlive = random() < 0.15 ? 1 : 0;
        let colorStart = isAlive ? rainbowPalette[0] : blackColor;
        newGrid[r][c] = {
          state:      isAlive,
          mem:        0,
          aliveCount: 0,
          everAlive:  isAlive,
          colorVal:   { r: colorStart.r, g: colorStart.g, b: colorStart.b }
        };
      }
    }
  }
  grid = newGrid;
}

// Basic 1D linear interpolation
function lerp(start, end, amt) {
  return (1 - amt) * start + amt * end;
}
</script>
</body>
</html>
